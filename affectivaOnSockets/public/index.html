<!DOCTYPE html>
<html lang="en">
 
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Stream My Aquarium</title>
 
  <!-- Bootstrap CSS -->
  <link href="http://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
 
  <style type="text/css">
  #stream {
    height: 30%;
    margin: 0px auto;
    display: block;
    margin-top: 20px;
  }
  </style>
 
  <!-- jQuery -->
  <script src="http://code.jquery.com/jquery.js"></script>
  <!-- Bootstrap JavaScript -->
  <script src="http://netdna.bootstrapcdn.com/bootstrap/3.1.1/js/bootstrap.min.js"></script>
  <script src="/socket.io/socket.io.js"></script>
  <script src="http://code.jquery.com/jquery.js"></script>
  <!-- Bootstrap JavaScript -->
  <script src="https://download.affectiva.com/js/3.1/affdex.js"></script>
  <link rel="stylesheet" href="./css/bootstrap.min.css">
  <link rel="stylesheet" href="./css/bootstrap-theme.min.css">
   <script>



    /*
     Face detector configuration - If not specified, defaults to F
     affdex.FaceDetectorMode.LARGE_FACES
     affdex.FaceDetectorMode.LARGE_FACES=Faces occupying large portions of the frame
     affdex.FaceDetectorMode.SMALL_FACES=Faces occupying small portions of the frame
  */
  var faceMode = affdex.FaceDetectorMode.SMALL_FACES;
  var isDetectorInitialized = false;
  //Cache the timestamp of the first frame processed
  var startTimestamp = (new Date()).getTime() / 1000;
  //Construct a FrameDetector and specify the image width / height and face detector mode.
  var detector = new affdex.FrameDetector(faceMode);

  // Track smiles
  detector.detectExpressions.smile = true;

  // Track joy emotion
  detector.detectEmotions.joy = true;

  // Detect person's gender
  detector.detectAppearance.gender = true;

  detector.width = 640;
  detector.height = 480;

  detector.detectAllExpressions();
  detector.detectAllEmotions();
  detector.detectAllEmojis();
  detector.detectAllAppearance();


  //Add a callback to notify when the detector is initialized and ready for runing.
  detector.addEventListener("onInitializeSuccess", function() {
    isDetectorInitialized = true;
    log('#logs', "The detector reports initialized");
    console.log("onInitializeSuccess");
    //Display canvas instead of video feed because we want to draw the feature points on it
    //$("#face_video_canvas").css("display", "block");
    //$("#face_video").css("display", "none");
    startTimestamp = (new Date()).getTime() / 1000;
    captureImage();
  });

  detector.addEventListener("onInitializeFailure", function() {
    isDetectorInitialized = false;
    log('#logs', "The detector reports initialization failed");
    console.log("onInitializeFailure");
  });

  /* 
    onImageResults success is called when a frame is processed successfully and receives 3 parameters:
    - Faces: Dictionary of faces in the frame keyed by the face id.
             For each face id, the values of detected emotions, expressions, appearane metrics 
             and coordinates of the feature points
    - image: An imageData object containing the pixel values for the processed frame.
    - timestamp: The timestamp of the captured image in seconds.
  */
  //detector.addEventListener("onImageResultsSuccess", function (faces, image, timestamp) {});

  /* 
    onImageResults success receives 3 parameters:
    - image: An imageData object containing the pixel values for the processed frame.
    - timestamp: An imageData object contain the pixel values for the processed frame.
    - err_detail: A string contains the encountered exception.
  */
  detector.addEventListener("onImageResultsFailure", function (image, timestamp, err_detail) {

    console.log("onImageResultsFailure");
    console.log("timestamp is "+ timestamp);
    console.log("err_detail " + err_detail);

    var c = document.getElementById("resultCanvas");
    var ctx = c.getContext("2d"); 
    ctx.putImageData(image, 10, 70);
  });


  //Add a callback to receive the results from processing an image.
  //The faces object contains the list of the faces detected in an image.
  //Faces object contains probabilities for all the different expressions, emotions and appearance metrics
  detector.addEventListener("onImageResultsSuccess", function(faces, image, timestamp) {
    console.log("onImageResultsSuccess" + timestamp);
    console.log("imagedata" + image);
    var c = document.getElementById("resultCanvas");
    var ctx = c.getContext("2d"); 
    ctx.putImageData(image, 10, 70);
    $('#results').html("");
    log('#results', "Timestamp: " + timestamp.toFixed(2));
    log('#results', "Number of faces found: " + faces.length);
    if (faces.length > 0) {
      log('#results', "Appearance: " + JSON.stringify(faces[0].appearance));
      log('#results', "Emotions: " + JSON.stringify(faces[0].emotions, function(key, val) {
        return val.toFixed ? Number(val.toFixed(0)) : val;
      }));
      log('#results', "Expressions: " + JSON.stringify(faces[0].expressions, function(key, val) {
        return val.toFixed ? Number(val.toFixed(0)) : val;
      }));
      log('#results', "Emoji: " + faces[0].emojis.dominantEmoji);
      //drawFeaturePoints(image, faces[0].featurePoints);

      
    }
  });

  detector.addEventListener("onResetSuccess", function() {});
  detector.addEventListener("onResetFailure", function() {});

  detector.addEventListener("onStopSuccess", function() {});
  detector.addEventListener("onStopFailure", function() {});

    //function executes when Start button is pushed.
   function startDetector() {
      console.log("startDetector");
      if (detector && !detector.isRunning) {
        $("#logs").html("");
        detector.start();
        console.log("detected started");
        //
      }else{
        $("#logs").html("Detector running");
      }
      log('#logs', "Clicked the start button");
   }


  

  function captureImage(){

     if(detector && detector.isRunning) {

       var c1 = document.createElement('canvas');
       var con1 = c1.getContext('2d');
       var stream = document.getElementById('stream');
       c1.width = stream.width;
       c1.height = stream.height;

      console.log("captureImage");
      //Get a canvas element from DOM
      
       con1.drawImage(stream, 0, 0 );
      //Get imageData object.
      var imageData = con1.getImageData(0, 0, stream.width, stream.height);

      //Get current time in seconds
      var now = (new Date()).getTime() / 1000;

      //Get delta time between the first frame and the current frame.
      var deltaTime = now - startTimestamp;
      console.log("deltaTime " + deltaTime);
      //Process the frame
      detector.process(imageData, deltaTime);

      // var c = document.getElementById("resultCanvas");
      // var ctx = c.getContext("2d");  
      // ctx.putImageData(imageData, 10, 70);
    }
  }

  function captureImageData(imageData){

    // console.log("imagedata" + image);
    // var c = document.getElementById("resultCanvas");
    // var ctx = c.getContext("2d"); 
    // ctx.putImageData(imageData, 10, 70);

    console.log("imagedata" + imageData);

    //Get current time in seconds
    var now = (new Date()).getTime() / 1000;

    //Get delta time between the first frame and the current frame.
    var deltaTime = now - startTimestamp;
    console.log("deltaTime " + deltaTime);
    //Process the frame
    detector.process(imageData, deltaTime);

  }

  function log(node_name, msg) {
   $(node_name).append("<span>" + msg + "</span><br />");
  }


  var socket = io();
  socket.on('liveStream', function(url) {
    $('#stream').attr('src', url);
    $('.start').hide();
    console.log("liveStream " + url);
    if(isDetectorInitialized){
      /*var canvas = document.createElement('canvas');
      var context = canvas.getContext('2d');
      var img = document.getElementById('stream');
      canvas.width = img.width;
      canvas.height = img.height;
      context.drawImage(img, 0, 0 );
      var imageData = context.getImageData(0, 0, img.width, img.height);
      captureImageData(imageData);*/
      captureImage();
      
    }
    
  });
 
  function startStream() {
    startDetector()
    socket.emit('start-stream');
    $('.start').hide();
  }

  function stopStream(){
    stopDetector();

  }

  function stopDetector() {
  console.log("Clicked the stop button");
  if (detector && detector.isRunning) {
    detector.removeEventListener();
    detector.stop();
  }
};

  </script>
</head>
 
<body class="container">
  <button type="button" id="" class="btn btn-info start" onclick="startStream()">Start Camera</button>
  <button type="button" id="" class="btn btn-info start" onclick="stopStream()">Stop Camera</button>
  <div class="row">
    <img  src="" id="stream"></img>
  </div>
  <div class="row">
      <div class="col-md-4">
        <canvas id ="resultCanvas" height="480" width="640"></canvas>
        <img src="" id ="resultImage"><img> 
      </div>
      <div class="col-md-4">
        <div style="height:25em;">
          <strong>RESULTS</strong>
          <div id="results" style="word-wrap:break-word;"></div>
        </div>
        <div>
          <strong>LOG MSGS</strong>
        </div>
        <div id="logs"></div>
      </div>
    </div>
 
</body>
 
</html>
